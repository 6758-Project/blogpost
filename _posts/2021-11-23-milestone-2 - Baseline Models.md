---
layout: post
title: NHL Data Science Project - Milestone 2 - Baseline Modeling
---


# Baseline Models

## Q1: Default Logistic Regression Classifier Prediction

The training and validation data are generated by applying a random 80/20 split at the game level to all game data between 2015/16 - 2018/19. A default logistic regression classifier is implmeneted as instructed. The model achieves an accuracy of ~90 when tested using the vlidation data. The confusion matrix, shown below, reveals that the output of the model is predominantly the "no-goal" label. The conclusion drawn from applying the cross validation method is that the predicted outcome is not affected by the shuffling of the data.

Such model behavior can be attributed to the overwhelmingly larger quantity of "no-goal" label than "goal" label at all distance values. In other words, the performance of the model is a result of the imbalanced dataset. Therefore, solely relying on the accruacy of the prediction is not an effective measurement of the model performance.

| Actual: No Goal |        6e+04       |        0        |
|   Actual: Goal  |        6222        |        0        |
|                 | Predicted: No Goal | Predicted: Goal |


## Q3: Model Performance Comparison

Three logistic regression models and a random baseline model are created and the performance results plotted for comparison (See figure below). It is noted that all three logistic regression models achieve the same accuracy when tested using the validation data.

The receiver operating characteristic (ROC) curve  plots the true positive rate of the prediction against the false positive rate and can be used to evaluate the accuracy of the model. A higher area under the curve (AUC) indicates the model is better at distinguishing between the two classes. Any curve that is above the baseline curve means that the proportion of correctly classified label is greater than the proportion of incorrectly classied label. Therefore, the ROC cruve suggests that the logistic regression model performance when using distance from net as input feature is similar to using distance from net and angle from net as input feature. Both of these models outperform the other two. Using only angle from net as input feature leads to a performance comparable if not worse than the random baseline. The curve suggests that the distance from net feature is more important than the angle from net feature.

The goal rate (#goals/(#no_goals + #goals)) against shot probability model percentile is shown below titled "True Positive Rate by Percentile". The extremes (percentile above 97.5) are removed for clarity. An ideal model would have a high true positive rate at a high probability percentile. In other words, the probability of correct prediction should align with the goal rate. Similar to the conclusion drawn from the ROC curve, this graph suggests that using distance or distance and angle as input features leads to a better perfoming model than using angle as input feature. In addition, the model trained using angle as input feature produces a lower true positive rate at a high probability percentile than the random baseline model. This observation further reduces the value of using the angle feature as an input feature.

The curve titled "Positive Proportion by Percentile" plots the cumulative proportion against the estimated probability percentile. The ideal model would reach a cumulative proportion of goals equal to 1 at a high probability percentile, only limited by the data distribution. The shape of this curve appears similar to the ROC curve, in this instance a similar conclusion can be drawn as well.

The reliability graph shows the relationship between the predicted probability and the empirical probability. In other words, it correlates the model generated probabilities of a true label with the actual fraction of the true label. The curve for the ideal model would overlap the perfectly calibrated line. The curve of the random baseline is horizontal because the model generated probability is uniformly distributed. The curves for the models trained on distance only and the model trained on distance and angle are similar, while they do overlap the perfectly calibrated line, the predicted probabilities are concentrated and never exceed 0.2. The model trained on angle only generated a vertical curve which indicates the variance of the predicted probability is very small. This curve suggests that the model is incapable of correlating the angle feature to the label. As such, the angle feature does not aid the logistic regression model in increasing prediction accuracy and performance.


![Model Confusion Matrix](/Images/M2_BM_Q3_VisualSummary.png)

### Comet.ml links:
Logistic regression classifier, trained on distance to net only: [baseline-logistic-regression-distance-only](https://www.comet.ml/tim-k-lee/model-registry/baseline-logistic-regression-distance-only)

Logistic regression classifier, trained on angle to net only: [baseline-logistic-regression-angle-only](https://www.comet.ml/tim-k-lee/model-registry/baseline-logistic-regression-angle-only)

Logistic regression classifier, trained on both distance and angle to net: [baseline-logistic-regression-distance-and-angle](https://www.comet.ml/tim-k-lee/model-registry/baseline-logistic-regression-distance-and-angle)






