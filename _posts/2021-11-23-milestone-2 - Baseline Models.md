---
layout: post
title: NHL Data Science Project - Milestone 2 - Baseline Modeling
---


# Baseline Models

## Q1: Default Logistic Regression Classifier Prediction

The training and validation data were generated by applying a random 80/20 split at the game level to all game data between 2015/16 - 2018/19. A default logistic regression classifier was implemented as instructed. The model achieved an accuracy of ~90 when tested using the validation data. The confusion matrix, shown below, reveals that the output of the model was predominantly the "no-goal" label. The conclusion drawn from applying the cross validation method was that the predicted outcome is not affected by the shuffling of the data.

Such model behavior can be attributed to the overwhelmingly large quantity of "no-goal" label compared to the "goal" label at all distance values. In other words, the performance of the model was a result of the imbalanced dataset. Therefore, solely relying on the accuracy of the prediction is not an effective measurement of the model performance.

| Actual: No Goal |        60164       |        0        |
|   Actual: Goal  |        6219        |        0        |
|                 | Predicted: No Goal | Predicted: Goal |


## Q3: Model Performance Comparison

Three logistic regression models and a random baseline model were created and the performance results plotted for comparison (See figure below). It was noted that all three logistic regression models achieved the same accuracy when tested using the validation data.

The receiver operating characteristic (ROC) curve plots the true positive rate of the prediction against the false positive rate and can be used as an alternative method to evaluate the accuracy of the model. A higher area under the curve (AUC) indicates the model is better at distinguishing between the two classes. Curves that lie above the baseline curve indicate the proportion of correctly classified labels is greater than the proportion of incorrectly classified labels. Therefore, the ROC curve suggests that the performance of the logistic regression model trained on distance from net is similar to the performance of the model trained on distance and angle from net. Both of these models outperformed the other two. Using only angle from net as input feature led to a performance comparable if not worse than the random baseline. The curves suggest that the distance from net feature is more important than the angle from net feature.

The goal rate (#goals/(#no_goals + #goals)) against shot probability model percentile is shown below titled "True Positive Rate by Percentile". The extremes (percentile above 97.5) were removed for clarity. An ideal model would have a high true positive rate at a high probability percentile. In other words, the probability of correct prediction should align with the goal rate. Similar to the conclusion drawn from the ROC curve, this graph suggests that using distance or distance and angle as input features led to a better performing model than using angle as input feature. In addition, the model trained using angle as input feature produced a lower true positive rate at high probability percentile than the random baseline model. This observation further reduces the value of using the angle feature as an input feature.

The curve titled "Positive Proportion by Percentile" plots the cumulative proportion against the estimated probability percentile. The ideal model would reach a cumulative proportion of goals equal to 1 at a high probability percentile, only limited by the data distribution. The shape of this curve appears similar to the ROC curve, in this instance a similar conclusion can be drawn as well.

The reliability graph shows the relationship between the predicted probability and the empirical probability. In other words, it correlates the model generated probabilities of a true label with the actual fraction of the true label. The curve for the ideal model would overlap the perfectly calibrated line. The curve of the random baseline is horizontal because the model generated probability is uniformly distributed. The curves for the models trained on distance only and the model trained on distance and angle are similar, while they do overlap the perfectly calibrated line, the predicted probabilities were concentrated and never exceed 0.2. The model trained on angle only generated a vertical curve which indicates the variance of the predicted probability is very small. This curve suggests that the model is incapable of correlating the angle feature to the label. As such, the angle feature did not aid the logistic regression model in increasing prediction accuracy and performance.


![Model Confusion Matrix](/Images/M2_BM_Q3_VisualSummary.png)

### Comet.ml links:
Logistic regression classifier, trained on distance to net only: [baseline-logistic-regression-distance-only](https://www.comet.ml/tim-k-lee/model-registry/baseline-logistic-regression-distance-only)

Logistic regression classifier, trained on angle to net only: [baseline-logistic-regression-angle-only](https://www.comet.ml/tim-k-lee/model-registry/baseline-logistic-regression-angle-only)

Logistic regression classifier, trained on both distance and angle to net: [baseline-logistic-regression-distance-and-angle](https://www.comet.ml/tim-k-lee/model-registry/baseline-logistic-regression-distance-and-angle)






